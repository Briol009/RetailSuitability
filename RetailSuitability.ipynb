{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22f9a0aa-3776-412b-bd00-a1e4c0048b4f",
   "metadata": {},
   "source": [
    "# MVP 2 Spatial Modelling and Analytics\n",
    "\n",
    "Laure Briol\n",
    "written with the help of Generative AI (ChatGPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c8999ab-4f61-4ff4-b0fb-ebd6002ae602",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements\n",
    "import os  # For file operations\n",
    "import requests  # For downloading files from websites\n",
    "import zipfile  # For unzipping downloaded files\n",
    "import overpy  # To get info from OpenStreetMap Overpass API\n",
    "import pandas as pd  # For data manipulation\n",
    "import geopandas as gpd  # For geospatial data manipulation\n",
    "from shapely.geometry import Point  # For creating point geometries\n",
    "import numpy as np  # For numerical operations\n",
    "import folium  # For creating interactive maps\n",
    "\n",
    "# Machine learning imports\n",
    "from sklearn.model_selection import train_test_split  # For splitting data\n",
    "from sklearn.preprocessing import MinMaxScaler  # For scaling features\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score  # For model evaluation\n",
    "from tensorflow.keras.models import Sequential  # For building neural network models\n",
    "from tensorflow.keras.layers import Dense  # For defining neural network layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a78e2fe6-d35a-47e1-b493-b70a556dcc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this MVP has a lot of code with it, so I decided to use 'functional programming'\n",
    "##each step of the process is pre-coded with a 'function'. \n",
    "##then, at the bottom I can run everything in one neat little code cell.\n",
    "\n",
    "#steps for this project:\n",
    "\n",
    "#1. download data -- this is downloading data from the minnesota geospatial commons\n",
    "## For this project we use: AADT -- annual average daily traffic data, ACS -- American community survey (demographic data), Census Tracts (population data)\n",
    "#2. get store locations -- this is using the overpass api to get the locations of a store across minnesota given the store name\n",
    "## To get locations of Target\n",
    "#3. acs and census -- this loads in annual community survey and census tract data\n",
    "## This will load the ACS data and merge it with census data. The ACS data is only in spreadsheet format, but we need to make it polygons\n",
    "#4. analysis grid -- this creates a standard 'grid'/raster format so it is easy to work with\n",
    "#5. data cleaning -- this takes in the data we downloaded and processes it for use with analysis, like averaging traffic within 1km of a point, or calculating population density\n",
    "#6. create neural network -- this is creating our 'ai' to decide where suitable business locations could be\n",
    "#7. evaluate model -- this is checking how accurate our model is at picking spots by comparing against existing store locations\n",
    "#8. predict location -- this is looking at every locaiton possible, and indicating if it is a 'suitable' locaiton for a store.\n",
    "#9. visualization -- this is creating a pretty map of the output and saving it to a file for later refrence\n",
    "\n",
    "\n",
    "# Step 1: Download and Extract Data\n",
    "\n",
    "#creates a function to download and extract a zip file from a given url\n",
    "def download_and_extract_zip(url, extract_to='data'):\n",
    "    #check if the file folder exists, if not, create it\n",
    "    if not os.path.exists(extract_to):\n",
    "        #create the folder\n",
    "        os.makedirs(extract_to)\n",
    "    \n",
    "    #get the filename from the URL\n",
    "    filename = url.split('/')[-1]\n",
    "    #create the full path for the zip file\n",
    "    zip_path = os.path.join(extract_to, filename)\n",
    "    \n",
    "    #check if the zip file already exists\n",
    "    if not os.path.exists(zip_path):\n",
    "        #print download message\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        #send a request to the url with streaming so it downloads when needed\n",
    "        response = requests.get(url, stream=True)\n",
    "        #check if the response status is 200 (sucessful download)\n",
    "        if response.status_code == 200:\n",
    "            #open the zip file\n",
    "            with open(zip_path, 'wb') as f:\n",
    "                #download the zip file the file in chunks of data\n",
    "                for chunk in response.iter_content(chunk_size=1024):\n",
    "                    #if there is a chunk of data, save it\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "            #print download completed message\n",
    "            print(\"Download completed.\")\n",
    "        else:\n",
    "            #error if download failed\n",
    "            raise Exception(f\"Failed to download file from {url}\")\n",
    "    else:\n",
    "        #print message that file already exists, if it was already downloaded\n",
    "        print(f\"{filename} already exists. Skipping download.\")\n",
    "    \n",
    "    #print extraction message\n",
    "    print(f\"Extracting {filename}...\")\n",
    "    #open the zip file\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        #unzip the file to the extract_to folder\n",
    "        zip_ref.extractall(extract_to)\n",
    "    #print extraction completed message\n",
    "    print(\"Extraction completed.\")\n",
    "    \n",
    "    #return the path to where all the data is downloaded\n",
    "    return extract_to\n",
    "\n",
    "\n",
    "# Step 2: Get store locations\n",
    "\n",
    "#creates a function to get store locations for a given business from openstreetmap using overpass api\n",
    "def get_store_locations(business_name, area_name=\"Minnesota\"):\n",
    "    #initialize the overpass api\n",
    "    api = overpy.Overpass()\n",
    "    #call the openstreetmap overpass api that lets us query specific map data\n",
    "    #nodes, ways, and relations are the basic data in openstreetmap\n",
    "    ##nodes represent individual points defined by their latitude and longitude\n",
    "    ##ways are ordered lists of nodes that define linear features like roads or boundaries\n",
    "    ##relations are collections of nodes, ways, and/or other relations that define more complex structures like routes or areas\n",
    "    #this defines the overpass query to get nodes, ways, and relations with the specified business name within the designated area\n",
    "    query = f\"\"\"\n",
    "    [out:json][timeout:60];\n",
    "    area[\"name\"=\"{area_name}\"]->.searchArea;\n",
    "    (\n",
    "      node[\"name\"=\"{business_name}\"](area.searchArea);\n",
    "      way[\"name\"=\"{business_name}\"](area.searchArea);\n",
    "      relation[\"name\"=\"{business_name}\"](area.searchArea);\n",
    "    );\n",
    "    out center;\n",
    "    \"\"\"\n",
    "    #try to execute the query\n",
    "    try:\n",
    "        result = api.query(query)\n",
    "    except Exception as e:\n",
    "        #print error message if the query fails\n",
    "        print(f\"Error fetching data for {business_name}: {e}\")\n",
    "        #return an empty geodataframe if there's an error\n",
    "        return gpd.GeoDataFrame(columns=['lat', 'lon', 'geometry'])\n",
    "    \n",
    "    #initialize a list to store location data\n",
    "    locations = []\n",
    "    #iterate over all returned nodes, ways, and relations\n",
    "    for element in result.nodes + result.ways + result.relations:\n",
    "        #get the latitude attribute if available\n",
    "        lat = getattr(element, 'lat', None)\n",
    "        #get the longitude attribute if available\n",
    "        lon = getattr(element, 'lon', None)\n",
    "        #if the element has center_lat and center_lon, use them\n",
    "        if hasattr(element, 'center_lat') and hasattr(element, 'center_lon'):\n",
    "            lat = element.center_lat\n",
    "            lon = element.center_lon\n",
    "        #if both latitude and longitude are available, add them to the locations list\n",
    "        if lat and lon:\n",
    "            locations.append({'lat': float(lat), 'lon': float(lon)})\n",
    "    \n",
    "    #check if any locations were found\n",
    "    if not locations:\n",
    "        #print message if no locations are found\n",
    "        print(f\"No locations found for {business_name}.\")\n",
    "        #return an empty geodataframe\n",
    "        return gpd.GeoDataFrame(columns=['lat', 'lon', 'geometry'])\n",
    "    \n",
    "    #create a dataframe from the locations list\n",
    "    df = pd.DataFrame(locations)\n",
    "    #convert the dataframe to a geodataframe with point geometries\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df, geometry=gpd.points_from_xy(df.lon, df.lat), crs=\"EPSG:4326\"\n",
    "    )\n",
    "    #return the geodataframe with store locations\n",
    "    return gdf\n",
    "\n",
    "\n",
    "# Step 3: Load ACS and Census Data\n",
    "\n",
    "#defines a function to load acs data from an excel file\n",
    "def load_acs_data(acs_excel_path):\n",
    "    #print loading message\n",
    "    print(\"Loading ACS data from Excel file...\")\n",
    "    #read the excel file into a dataframe\n",
    "    acs_df = pd.read_excel(acs_excel_path)\n",
    "    #print the number of records loaded\n",
    "    print(f\"Loaded ACS data with {len(acs_df)} records.\")\n",
    "    #return the acs dataframe\n",
    "    return acs_df\n",
    "\n",
    "#defines a function to load census tract shapefile\n",
    "def load_census_tract_shapefile(shapefile_path):\n",
    "    #print loading message\n",
    "    print(\"Loading Census Tract shapefile...\")\n",
    "    #read the shapefile into a geodataframe\n",
    "    tracts_gdf = gpd.read_file(shapefile_path)\n",
    "    #print the number of records loaded\n",
    "    print(f\"Loaded Census Tract shapefile with {len(tracts_gdf)} records.\")\n",
    "    #return the census tracts geodataframe\n",
    "    return tracts_gdf\n",
    "\n",
    "#defines a function to integrate acs data with census tract geometries\n",
    "def integrate_acs_with_tracts(acs_df, tracts_gdf):\n",
    "    #ensure GEOID2 column in acs_df is of string type\n",
    "    acs_df['GEOID2'] = acs_df['GEOID2'].astype(str)\n",
    "    #ensure GEOID20 column in tracts_gdf is of string type\n",
    "    tracts_gdf['GEOID20'] = tracts_gdf['GEOID20'].astype(str)\n",
    "    \n",
    "    #print integration message\n",
    "    print(\"Integrating ACS data with Census Tract geometries...\")\n",
    "    #merge the tracts geodataframe with the acs dataframe on GEOID columns\n",
    "    merged_gdf = tracts_gdf.merge(\n",
    "        acs_df, left_on='GEOID20', right_on='GEOID2', how='left', suffixes=('', '_drop')\n",
    "    )\n",
    "    #drop any columns that have '_drop' suffix from the merge\n",
    "    merged_gdf = merged_gdf[[col for col in merged_gdf.columns if not col.endswith('_drop')]]\n",
    "    \n",
    "    #print the number of records after merging\n",
    "    print(f\"Merged data has {len(merged_gdf)} records.\")\n",
    "    #return the merged geodataframe\n",
    "    return merged_gdf\n",
    "\n",
    "\n",
    "# Step 4: Create Analysis Grid\n",
    "\n",
    "#defines a function to create a grid of points over a specified bounding box\n",
    "def create_analysis_grid(minx, miny, maxx, maxy, grid_spacing=0.01):\n",
    "    #create a range of x coordinates based on grid spacing\n",
    "    x_coords = np.arange(minx, maxx, grid_spacing)\n",
    "    #create a range of y coordinates based on grid spacing\n",
    "    y_coords = np.arange(miny, maxy, grid_spacing)\n",
    "    #create point geometries for each combination of x and y coordinates\n",
    "    grid_points = [Point(x, y) for x in x_coords for y in y_coords]\n",
    "    #create a geodataframe from the grid points with the specified CRS\n",
    "    grid_gdf = gpd.GeoDataFrame(geometry=grid_points, crs=\"EPSG:4326\")\n",
    "    #print the number of grid points generated\n",
    "    print(f\"Generated {len(grid_gdf)} grid points.\")\n",
    "    #return the grid geodataframe\n",
    "    return grid_gdf\n",
    "\n",
    "\n",
    "# Step 5: Data Cleaning\n",
    "\n",
    "#defines a function to calculate average traffic volume within a specified radius around each grid point\n",
    "def calculate_average_traffic(grid, traffic_data, radius=1000, projection_epsg=26915):\n",
    "    #print calculation message\n",
    "    print(\"Calculating average traffic volume within 1 km radius...\")\n",
    "    #project traffic data to the specified EPSG for accurate distance calculations\n",
    "    traffic_proj = traffic_data.to_crs(epsg=projection_epsg)\n",
    "    #project grid data to the specified EPSG\n",
    "    grid_proj = grid.to_crs(epsg=projection_epsg)\n",
    "    \n",
    "    #initialize a list to store average traffic values\n",
    "    average_traffic = []\n",
    "    #create a spatial index for the projected traffic data for faster queries\n",
    "    traffic_sindex = traffic_proj.sindex\n",
    "    \n",
    "    #iterate over each point in the projected grid\n",
    "    for idx, point in enumerate(grid_proj.geometry):\n",
    "        #create a buffer of the specified radius around the point\n",
    "        buffer = point.buffer(radius)\n",
    "        #find possible matches within the buffer bounds using the spatial index\n",
    "        possible_matches_index = list(traffic_sindex.intersection(buffer.bounds))\n",
    "        #select the possible matching traffic records\n",
    "        possible_matches = traffic_proj.iloc[possible_matches_index]\n",
    "        #select the precise matches that actually intersect with the buffer\n",
    "        precise_matches = possible_matches[possible_matches.intersects(buffer)]\n",
    "        \n",
    "        #if there are precise matches, calculate the average traffic volume\n",
    "        if not precise_matches.empty:\n",
    "            avg_volume = precise_matches['CURRENT_VO'].mean()\n",
    "        else:\n",
    "            #if no matches, set average traffic to 0\n",
    "            avg_volume = 0\n",
    "        \n",
    "        #append the average traffic to the list\n",
    "        average_traffic.append(avg_volume)\n",
    "        \n",
    "        #print progress every 500 grid points\n",
    "        if idx % 500 == 0 and idx > 0:\n",
    "            print(f\"Processed {idx} grid points...\")\n",
    "    \n",
    "    #add the average traffic data to the grid geodataframe\n",
    "    grid['average_traffic'] = average_traffic\n",
    "    #print completion message\n",
    "    print(\"Average traffic calculation completed.\")\n",
    "    #return the updated grid geodataframe\n",
    "    return grid\n",
    "\n",
    "#defines a function to assign acs data to each grid point based on the census tract it falls into\n",
    "def assign_acs_data_to_grid(grid, acs_gdf):\n",
    "    #print assignment message\n",
    "    print(\"Assigning ACS data to grid points...\")\n",
    "    #project acs geodataframe to match grid CRS\n",
    "    acs_gdf = acs_gdf.to_crs(grid.crs)\n",
    "    #perform a spatial join to assign acs data to grid points within census tracts\n",
    "    grid_with_acs = gpd.sjoin(grid, acs_gdf, how='left', op='within')\n",
    "    #fill any missing values with 0\n",
    "    grid_with_acs.fillna(0, inplace=True)\n",
    "    #print completion message\n",
    "    print(\"Assignment completed.\")\n",
    "    #return the grid geodataframe with assigned ACS data\n",
    "    return grid_with_acs\n",
    "\n",
    "#defines a function to calculate additional features like population density and normalized income\n",
    "def calculate_additional_features(gdf, acs_tracts_gdf):\n",
    "    #calculate area in square kilometers by projecting to a suitable CRS and computing geometry area\n",
    "    acs_tracts_gdf['area_sqkm'] = acs_tracts_gdf.to_crs(epsg=26915).geometry.area / 1e6\n",
    "    \n",
    "    #identify the correct population column name\n",
    "    population_col = 'POPTOTAL' if 'POPTOTAL' in acs_tracts_gdf.columns else 'TOTAL_POP'\n",
    "    #identify the correct median household income column name\n",
    "    median_income_col = 'MEDIANHHI' if 'MEDIANHHI' in acs_tracts_gdf.columns else 'MED_HH_INC'\n",
    "    #identify the correct drove alone column name\n",
    "    drove_alone_col = 'DROVEALONE' if 'DROVEALONE' in acs_tracts_gdf.columns else 'DRIVE_ALONE'\n",
    "    #identify the correct work denominator column name\n",
    "    work_denom_col = 'WORKDENOM' if 'WORKDENOM' in acs_tracts_gdf.columns else 'WORK_DENOM'\n",
    "    #identify the correct public transit column name\n",
    "    pub_transit_col = 'PUBTRANSIT' if 'PUBTRANSIT' in acs_tracts_gdf.columns else 'PUBLIC_TRANS'\n",
    "    #identify the correct walk to work column name\n",
    "    walk_to_work_col = 'WALKTOWORK' if 'WALKTOWORK' in acs_tracts_gdf.columns else 'WALK_TO_WORK'\n",
    "    \n",
    "    #print feature calculation message\n",
    "    print(\"Calculating additional features like population density and normalized income...\")\n",
    "    #select relevant columns for merging\n",
    "    area_pop_df = acs_tracts_gdf[['GEOID20', 'area_sqkm', population_col, median_income_col, drove_alone_col, work_denom_col, pub_transit_col, walk_to_work_col, 'HOMEOWNPCT', 'POVERTYN']]\n",
    "    #merge the selected ACS data with the grid geodataframe\n",
    "    gdf = gdf.merge(area_pop_df, left_on='GEOID20', right_on='GEOID20', how='left', suffixes=('', '_drop'))\n",
    "    #drop any columns with '_drop' suffix resulting from the merge\n",
    "    gdf = gdf[[col for col in gdf.columns if not col.endswith('_drop')]]\n",
    "    \n",
    "    #calculate population density by dividing total population by area in square kilometers\n",
    "    gdf['population_density'] = gdf[population_col] / gdf['area_sqkm']\n",
    "    \n",
    "    #normalize median household income between 0 and 1\n",
    "    gdf['median_hhi_norm'] = (gdf[median_income_col] - acs_tracts_gdf[median_income_col].min()) / (acs_tracts_gdf[median_income_col].max() - acs_tracts_gdf[median_income_col].min())\n",
    "    \n",
    "    #calculate the percentage of people who drive alone to work\n",
    "    gdf['perc_drive_alone'] = gdf[drove_alone_col] / gdf[work_denom_col]\n",
    "    #calculate the percentage of people who use public transit to work\n",
    "    gdf['perc_public_transit'] = gdf[pub_transit_col] / gdf[work_denom_col]\n",
    "    #calculate the percentage of people who walk to work\n",
    "    gdf['perc_walk_to_work'] = gdf[walk_to_work_col] / gdf[work_denom_col]\n",
    "    \n",
    "    #replace any infinite values with 0\n",
    "    gdf.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    #fill any remaining missing values with 0\n",
    "    gdf.fillna(0, inplace=True)\n",
    "    \n",
    "    #return the geodataframe with additional features\n",
    "    return gdf\n",
    "\n",
    "#defines a function to extract features at store locations based on ACS and AADT data\n",
    "def extract_features_at_locations(locations_gdf, acs_gdf, aadt_gdf):\n",
    "    #ensure the CRS of acs and aadt data matches the locations geodataframe\n",
    "    acs_gdf = acs_gdf.to_crs(locations_gdf.crs)\n",
    "    aadt_gdf = aadt_gdf.to_crs(locations_gdf.crs)\n",
    "    \n",
    "    #perform a spatial join to assign ACS data to store locations\n",
    "    locations_with_acs = gpd.sjoin(locations_gdf, acs_gdf, how='left', op='within')\n",
    "    #fill any missing values with 0\n",
    "    locations_with_acs.fillna(0, inplace=True)\n",
    "    \n",
    "    #calculate average traffic near each location within a 1 km radius\n",
    "    locations_with_features = calculate_average_traffic(locations_with_acs, aadt_gdf, radius=1000)\n",
    "    \n",
    "    #return the locations geodataframe with extracted features\n",
    "    return locations_with_features\n",
    "\n",
    "#defines a function to generate negative samples from grid points not containing existing store locations\n",
    "def generate_negative_samples(grid_gdf, num_samples):\n",
    "    #randomly sample grid points equal to the number of negative samples needed\n",
    "    negative_samples = grid_gdf.sample(n=num_samples, random_state=42)\n",
    "    #return the negative samples geodataframe\n",
    "    return negative_samples\n",
    "\n",
    "#defines a function to prepare training data by combining positive and negative samples\n",
    "def prepare_training_data(existing_features, negative_samples, features):\n",
    "    #assign label 1 to existing store locations\n",
    "    existing_features['label'] = 1\n",
    "    #assign label 0 to negative samples\n",
    "    negative_samples['label'] = 0\n",
    "    \n",
    "    #ensure both datasets have the same feature columns and the label\n",
    "    existing_features = existing_features[features + ['label']]\n",
    "    negative_samples = negative_samples[features + ['label']]\n",
    "    \n",
    "    #combine the positive and negative samples into one training dataset\n",
    "    training_data = pd.concat([existing_features, negative_samples], ignore_index=True)\n",
    "    \n",
    "    #return the combined training data\n",
    "    return training_data\n",
    "\n",
    "\n",
    "# Step 6: Build and Train Neural Network\n",
    "\n",
    "#defines a function to build and train a simple neural network model\n",
    "def build_and_train_nn(X_train, y_train, input_dim, epochs=50, batch_size=16):\n",
    "    #print building message\n",
    "    print(\"Building the neural network model...\")\n",
    "    #define a sequential neural network model\n",
    "    model = Sequential([\n",
    "        #add a dense layer with 32 neurons and relu activation\n",
    "        Dense(32, activation='relu', input_shape=(input_dim,)),\n",
    "        #add a dense layer with 16 neurons and relu activation\n",
    "        Dense(16, activation='relu'),\n",
    "        #add an output dense layer with sigmoid activation for binary classification\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    #compile the model with adam optimizer and binary crossentropy loss\n",
    "    #this is to pull everything together\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    #print training message\n",
    "    print(\"Training the neural network model...\")\n",
    "    #train the model on the training data with specified epochs and batch size\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    #print completion message\n",
    "    print(\"Training completed.\")\n",
    "    #return the trained model and training history\n",
    "    return model, history\n",
    "\n",
    "\n",
    "# Step 7: Evaluate Model\n",
    "\n",
    "#defines a function to compute permutation importance for model features\n",
    "def compute_permutation_importance(model, X_test, y_test, features, scoring='accuracy', n_repeats=5, random_state=42):\n",
    "    #set the random seed for reproducibility\n",
    "    np.random.seed(random_state)\n",
    "    #get model predictions on the test set\n",
    "    baseline_preds = model.predict(X_test).flatten()\n",
    "    #convert probabilities to binary predictions\n",
    "    baseline_preds_class = (baseline_preds >= 0.5).astype(int)\n",
    "    \n",
    "    #calculate the baseline score based on the chosen scoring method\n",
    "    if scoring == 'accuracy':\n",
    "        baseline_score = accuracy_score(y_test, baseline_preds_class)\n",
    "    elif scoring == 'f1':\n",
    "        baseline_score = f1_score(y_test, baseline_preds_class)\n",
    "    else:\n",
    "        #raise an error if an unsupported scoring method is chosen\n",
    "        raise ValueError('Unsupported scoring method')\n",
    "    \n",
    "    #initialize a list to store importance scores\n",
    "    importances = []\n",
    "    #iterate over each feature column\n",
    "    for col in range(X_test.shape[1]):\n",
    "        #initialize a list to store scores for each repeat\n",
    "        scores = []\n",
    "        #repeat the permutation process n_repeats times\n",
    "        for _ in range(n_repeats):\n",
    "            #copy the test set\n",
    "            X_permuted = X_test.copy()\n",
    "            #shuffle the values in the current feature column\n",
    "            np.random.shuffle(X_permuted[:, col])\n",
    "            #get model predictions on the permuted test set\n",
    "            permuted_preds = model.predict(X_permuted).flatten()\n",
    "            #convert probabilities to binary predictions\n",
    "            permuted_preds_class = (permuted_preds >= 0.5).astype(int)\n",
    "            #calculate the score based on the chosen scoring method\n",
    "            if scoring == 'accuracy':\n",
    "                score = accuracy_score(y_test, permuted_preds_class)\n",
    "            elif scoring == 'f1':\n",
    "                score = f1_score(y_test, permuted_preds_class)\n",
    "            #append the difference from the baseline score\n",
    "            scores.append(baseline_score - score)\n",
    "        #calculate the mean importance score for the current feature\n",
    "        importances.append(np.mean(scores))\n",
    "    #create a dataframe with feature names and their importance scores\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': features,\n",
    "        'importance_mean': importances\n",
    "    }).sort_values(by='importance_mean', ascending=False)\n",
    "    #return the feature importance dataframe\n",
    "    return importance_df\n",
    "\n",
    "#defines a function to evaluate the trained model on test data and print classification report\n",
    "def evaluate_model(model, X_test, y_test, features):\n",
    "    #print evaluation message\n",
    "    print(\"Evaluating the model on test data...\")\n",
    "    #evaluate the model and get loss and accuracy\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    #print the test accuracy\n",
    "    print(f'\\nTest Accuracy: {accuracy:.2f}')\n",
    "    \n",
    "    #get model prediction probabilities on the test set\n",
    "    y_pred_prob = model.predict(X_test).flatten()\n",
    "    #convert probabilities to binary predictions\n",
    "    y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "    \n",
    "    #print the classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    #compute feature importances using permutation importance\n",
    "    importance_df = compute_permutation_importance(model, X_test, y_test, features)\n",
    "    \n",
    "    #print the feature importances\n",
    "    print(\"\\nFeature Importances:\")\n",
    "    print(importance_df)\n",
    "    \n",
    "    #return the feature importance dataframe\n",
    "    return importance_df\n",
    "\n",
    "# Step 8: Predict Locations\n",
    "\n",
    "#defines a function to predict suitability for all grid points using the trained model\n",
    "def predict_suitable_locations(model, scaler, grid, features):\n",
    "    #print prediction message\n",
    "    print(\"Predicting suitability for all grid points...\")\n",
    "    #extract feature values from the grid geodataframe\n",
    "    X_grid = grid[features].values\n",
    "    #scale the feature values using the provided scaler\n",
    "    X_grid_scaled = scaler.transform(X_grid)\n",
    "    #get model prediction probabilities for the grid points\n",
    "    predictions = model.predict(X_grid_scaled).flatten()\n",
    "    #add the prediction probabilities to the grid geodataframe\n",
    "    grid['prediction'] = predictions\n",
    "    #assign suitability based on a threshold of 0.5\n",
    "    grid['suitable'] = (grid['prediction'] >= 0.5).astype(int)\n",
    "    #print the number of suitable locations identified\n",
    "    print(f\"Identified {grid['suitable'].sum()} suitable locations.\")\n",
    "    #return the updated grid geodataframe with predictions\n",
    "    return grid\n",
    "\n",
    "\n",
    "# Step 9: Visualization\n",
    "\n",
    "#defines a function to create an interactive folium map showing existing stores and suitable locations\n",
    "def visualize_results(stores_gdf, suitable_gdf, map_filename='retail_suitability_map.html'):\n",
    "    #print visualization message\n",
    "    print(\"Creating interactive map for visualization...\")\n",
    "    #define the center of the map\n",
    "    map_center = [44.96, -93.1]\n",
    "    #create a folium map centered at the specified location\n",
    "    m = folium.Map(location=map_center, zoom_start=10)\n",
    "    \n",
    "    #iterate over each existing store location\n",
    "    for idx, row in stores_gdf.iterrows():\n",
    "        #add a blue shopping cart marker for each existing store\n",
    "        folium.Marker(\n",
    "            [row.geometry.y, row.geometry.x],\n",
    "            popup='Existing Store',\n",
    "            icon=folium.Icon(color='blue', icon='shopping-cart', prefix='fa')\n",
    "        ).add_to(m)\n",
    "    \n",
    "    #iterate over each suitable location\n",
    "    for idx, row in suitable_gdf.iterrows():\n",
    "        #add a green circle marker for each suitable location\n",
    "        folium.CircleMarker(\n",
    "            location=[row.geometry.y, row.geometry.x],\n",
    "            radius=2,\n",
    "            color='green',\n",
    "            fill=True,\n",
    "            fill_color='green',\n",
    "            fill_opacity=0.5,\n",
    "            popup='Potential Site'\n",
    "        ).add_to(m)\n",
    "    \n",
    "    #add layer control to the map\n",
    "    folium.LayerControl().add_to(m)\n",
    "    #save the map to an HTML file\n",
    "    m.save(map_filename)\n",
    "    #print completion message with the filename\n",
    "    print(f\"Map has been saved to '{map_filename}'.\")\n",
    "    #inform the user to open the map in a web browser\n",
    "    print(\"You can open the map in a web browser to view the results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cb87a39-1c4b-4f3f-bd78-1f939c0cc5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xlsx_society_census_acs.zip already exists. Skipping download.\n",
      "Extracting xlsx_society_census_acs.zip...\n",
      "Extraction completed.\n",
      "shp_society_census2020tiger.zip already exists. Skipping download.\n",
      "Extracting shp_society_census2020tiger.zip...\n",
      "Extraction completed.\n",
      "shp_trans_aadt_traffic_count_locs.zip already exists. Skipping download.\n",
      "Extracting shp_trans_aadt_traffic_count_locs.zip...\n",
      "Extraction completed.\n",
      "Fetching locations for Target...\n",
      "Found 78 locations for Target.\n",
      "Loading ACS data from Excel file...\n",
      "Loaded ACS data with 1545 records.\n",
      "Loading Census Tract shapefile...\n",
      "Loaded Census Tract shapefile with 784 records.\n",
      "Integrating ACS data with Census Tract geometries...\n",
      "Merged data has 784 records.\n",
      "Creating analysis grid...\n",
      "Generated 5670 grid points.\n",
      "Assigning ACS data to grid points...\n",
      "Assignment completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Logan\\AppData\\Local\\Temp\\ipykernel_20912\\3946954671.py:194: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  main()\n",
      "C:\\Users\\Logan\\AppData\\Local\\Temp\\ipykernel_20912\\3946954671.py:194: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  main()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating average traffic volume within 1 km radius...\n",
      "Average traffic calculation completed.\n",
      "Calculating additional features like population density and normalized income...\n",
      "Calculating average traffic volume within 1 km radius...\n",
      "Average traffic calculation completed.\n",
      "Calculating additional features like population density and normalized income...\n",
      "Preparing data for modeling...\n",
      "Training samples: 374, Testing samples: 94\n",
      "Building the neural network model...\n",
      "Training the neural network model...\n",
      "Epoch 1/50\n",
      "21/21 [==============================] - 1s 18ms/step - loss: 0.5886 - accuracy: 0.8482 - val_loss: 0.5354 - val_accuracy: 0.8158\n",
      "Epoch 2/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.4712 - accuracy: 0.8363 - val_loss: 0.4575 - val_accuracy: 0.8158\n",
      "Epoch 3/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.4136 - accuracy: 0.8363 - val_loss: 0.4276 - val_accuracy: 0.8158\n",
      "Epoch 4/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.3880 - accuracy: 0.8363 - val_loss: 0.4196 - val_accuracy: 0.8158\n",
      "Epoch 5/50\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 0.3755 - accuracy: 0.8363 - val_loss: 0.4139 - val_accuracy: 0.8158\n",
      "Epoch 6/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.3617 - accuracy: 0.8452 - val_loss: 0.4140 - val_accuracy: 0.8947\n",
      "Epoch 7/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.3520 - accuracy: 0.8661 - val_loss: 0.4109 - val_accuracy: 0.8947\n",
      "Epoch 8/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.3444 - accuracy: 0.8720 - val_loss: 0.4132 - val_accuracy: 0.8421\n",
      "Epoch 9/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.3354 - accuracy: 0.8750 - val_loss: 0.4122 - val_accuracy: 0.8421\n",
      "Epoch 10/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.3301 - accuracy: 0.8750 - val_loss: 0.4124 - val_accuracy: 0.8421\n",
      "Epoch 11/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.3250 - accuracy: 0.8690 - val_loss: 0.4164 - val_accuracy: 0.8421\n",
      "Epoch 12/50\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 0.3227 - accuracy: 0.8690 - val_loss: 0.4177 - val_accuracy: 0.8421\n",
      "Epoch 13/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.3237 - accuracy: 0.8750 - val_loss: 0.4170 - val_accuracy: 0.8421\n",
      "Epoch 14/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.3159 - accuracy: 0.8690 - val_loss: 0.4226 - val_accuracy: 0.8158\n",
      "Epoch 15/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.3162 - accuracy: 0.8631 - val_loss: 0.4208 - val_accuracy: 0.8421\n",
      "Epoch 16/50\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 0.3163 - accuracy: 0.8601 - val_loss: 0.4228 - val_accuracy: 0.8421\n",
      "Epoch 17/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.3145 - accuracy: 0.8661 - val_loss: 0.4213 - val_accuracy: 0.8421\n",
      "Epoch 18/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.3112 - accuracy: 0.8661 - val_loss: 0.4240 - val_accuracy: 0.8421\n",
      "Epoch 19/50\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 0.3103 - accuracy: 0.8690 - val_loss: 0.4235 - val_accuracy: 0.8421\n",
      "Epoch 20/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.3107 - accuracy: 0.8661 - val_loss: 0.4253 - val_accuracy: 0.8158\n",
      "Epoch 21/50\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 0.3137 - accuracy: 0.8690 - val_loss: 0.4235 - val_accuracy: 0.8421\n",
      "Epoch 22/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.3077 - accuracy: 0.8601 - val_loss: 0.4275 - val_accuracy: 0.8158\n",
      "Epoch 23/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.3073 - accuracy: 0.8631 - val_loss: 0.4266 - val_accuracy: 0.8158\n",
      "Epoch 24/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.3077 - accuracy: 0.8690 - val_loss: 0.4265 - val_accuracy: 0.8158\n",
      "Epoch 25/50\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 0.3050 - accuracy: 0.8661 - val_loss: 0.4296 - val_accuracy: 0.8158\n",
      "Epoch 26/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.3064 - accuracy: 0.8601 - val_loss: 0.4266 - val_accuracy: 0.8158\n",
      "Epoch 27/50\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 0.3063 - accuracy: 0.8542 - val_loss: 0.4272 - val_accuracy: 0.8158\n",
      "Epoch 28/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.3037 - accuracy: 0.8720 - val_loss: 0.4285 - val_accuracy: 0.8158\n",
      "Epoch 29/50\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 0.3038 - accuracy: 0.8720 - val_loss: 0.4267 - val_accuracy: 0.8158\n",
      "Epoch 30/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.3037 - accuracy: 0.8661 - val_loss: 0.4282 - val_accuracy: 0.8158\n",
      "Epoch 31/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.3054 - accuracy: 0.8690 - val_loss: 0.4287 - val_accuracy: 0.8158\n",
      "Epoch 32/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.3061 - accuracy: 0.8482 - val_loss: 0.4335 - val_accuracy: 0.8158\n",
      "Epoch 33/50\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 0.3013 - accuracy: 0.8750 - val_loss: 0.4270 - val_accuracy: 0.8158\n",
      "Epoch 34/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.3038 - accuracy: 0.8601 - val_loss: 0.4283 - val_accuracy: 0.8158\n",
      "Epoch 35/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.3013 - accuracy: 0.8690 - val_loss: 0.4297 - val_accuracy: 0.8158\n",
      "Epoch 36/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.2989 - accuracy: 0.8720 - val_loss: 0.4294 - val_accuracy: 0.8158\n",
      "Epoch 37/50\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 0.2984 - accuracy: 0.8720 - val_loss: 0.4304 - val_accuracy: 0.8158\n",
      "Epoch 38/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.2982 - accuracy: 0.8690 - val_loss: 0.4308 - val_accuracy: 0.8158\n",
      "Epoch 39/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.2992 - accuracy: 0.8601 - val_loss: 0.4271 - val_accuracy: 0.8158\n",
      "Epoch 40/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.2971 - accuracy: 0.8720 - val_loss: 0.4289 - val_accuracy: 0.8158\n",
      "Epoch 41/50\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 0.2960 - accuracy: 0.8690 - val_loss: 0.4300 - val_accuracy: 0.8158\n",
      "Epoch 42/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.2972 - accuracy: 0.8631 - val_loss: 0.4303 - val_accuracy: 0.8158\n",
      "Epoch 43/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.2966 - accuracy: 0.8631 - val_loss: 0.4303 - val_accuracy: 0.8158\n",
      "Epoch 44/50\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 0.2945 - accuracy: 0.8720 - val_loss: 0.4288 - val_accuracy: 0.8158\n",
      "Epoch 45/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.2944 - accuracy: 0.8750 - val_loss: 0.4283 - val_accuracy: 0.8158\n",
      "Epoch 46/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.2942 - accuracy: 0.8631 - val_loss: 0.4318 - val_accuracy: 0.8158\n",
      "Epoch 47/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.2944 - accuracy: 0.8661 - val_loss: 0.4281 - val_accuracy: 0.8158\n",
      "Epoch 48/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.2943 - accuracy: 0.8690 - val_loss: 0.4319 - val_accuracy: 0.8158\n",
      "Epoch 49/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.2929 - accuracy: 0.8661 - val_loss: 0.4296 - val_accuracy: 0.8158\n",
      "Epoch 50/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 0.2947 - accuracy: 0.8631 - val_loss: 0.4333 - val_accuracy: 0.8158\n",
      "Training completed.\n",
      "Evaluating the model on test data...\n",
      "\n",
      "Test Accuracy: 0.86\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.92        78\n",
      "           1       0.59      0.62      0.61        16\n",
      "\n",
      "    accuracy                           0.86        94\n",
      "   macro avg       0.76      0.77      0.76        94\n",
      "weighted avg       0.87      0.86      0.86        94\n",
      "\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "\n",
      "Feature Importances:\n",
      "               feature  importance_mean\n",
      "0      average_traffic         0.048936\n",
      "6           HOMEOWNPCT         0.023404\n",
      "3     perc_drive_alone         0.012766\n",
      "7             POVERTYN         0.010638\n",
      "1   population_density         0.008511\n",
      "5    perc_walk_to_work         0.002128\n",
      "4  perc_public_transit         0.000000\n",
      "2      median_hhi_norm        -0.002128\n",
      "Calculating average traffic volume within 1 km radius...\n",
      "Processed 500 grid points...\n",
      "Processed 1000 grid points...\n",
      "Processed 1500 grid points...\n",
      "Processed 2000 grid points...\n",
      "Processed 2500 grid points...\n",
      "Processed 3000 grid points...\n",
      "Processed 3500 grid points...\n",
      "Processed 4000 grid points...\n",
      "Processed 4500 grid points...\n",
      "Processed 5000 grid points...\n",
      "Processed 5500 grid points...\n",
      "Average traffic calculation completed.\n",
      "Calculating additional features like population density and normalized income...\n",
      "Predicting suitability for all grid points...\n",
      "178/178 [==============================] - 0s 2ms/step\n",
      "Identified 411 suitable locations.\n",
      "Identified 411 suitable locations.\n",
      "Creating interactive map for visualization...\n",
      "Map has been saved to 'retail_suitability_map.html'.\n",
      "You can open the map in a web browser to view the results.\n",
      "Retail Site Suitability Analysis completed successfully.\n"
     ]
    }
   ],
   "source": [
    "#Main function. this function is what is run when I want to make the computer do all of the predictions and everything\n",
    "##I give some setup information, and it calls the functions that I created above to run the code\n",
    "##I pre-defined the functions above so that this code is much shorter looking and more neat!\n",
    "\n",
    "#defines the main function that runs everything together\n",
    "def main():\n",
    "\n",
    "    # Step 1: Download and Extract Data\n",
    "\n",
    "    #set the url for acs data\n",
    "    acs_data_url = \"https://resources.gisdata.mn.gov/pub/gdrs/data/pub/us_mn_state_metc/society_census_acs/xlsx_society_census_acs.zip\"\n",
    "    #set the url for census tract shapefile\n",
    "    tract_shapefile_url = \"https://resources.gisdata.mn.gov/pub/gdrs/data/pub/us_mn_state_metc/society_census2020tiger/shp_society_census2020tiger.zip\"\n",
    "    #set the url for aadt data\n",
    "    aadt_url = \"https://resources.gisdata.mn.gov/pub/gdrs/data/pub/us_mn_state_dot/trans_aadt_traffic_count_locs/shp_trans_aadt_traffic_count_locs.zip\"\n",
    "    \n",
    "    #set the extraction directory for acs data\n",
    "    acs_extract_dir = 'data/acs'\n",
    "    #download and extract acs data to the specified directory\n",
    "    download_and_extract_zip(acs_data_url, extract_to=acs_extract_dir)\n",
    "    #set the path to the acs excel file\n",
    "    acs_excel_path = os.path.join(acs_extract_dir, 'CensusACSTract.xlsx')\n",
    "    \n",
    "    #set the extraction directory for census tract shapefile\n",
    "    tract_extract_dir = 'data/census_tracts'\n",
    "    #download and extract census tract shapefile to the specified directory\n",
    "    download_and_extract_zip(tract_shapefile_url, extract_to=tract_extract_dir)\n",
    "    #set the path to the census tract shapefile\n",
    "    tract_shapefile_path = os.path.join(tract_extract_dir, 'Census2020TigerTract.shp')\n",
    "    \n",
    "    #set the extraction directory for aadt data\n",
    "    aadt_extract_dir = 'data/aadt'\n",
    "    #download and extract aadt data to the specified directory\n",
    "    download_and_extract_zip(aadt_url, extract_to=aadt_extract_dir)\n",
    "    #set the path to the aadt shapefile\n",
    "    aadt_shapefile_path = os.path.join(aadt_extract_dir, 'Annual_Average_Daily_Traffic_Locations_in_Minnesota.shp')\n",
    "\n",
    "    \n",
    "    # Step 2: Get store locaitons\n",
    "\n",
    "    #define a list of business names to fetch locations for\n",
    "    businesses = [\"Target\"]\n",
    "    #initialize a dictionary to store business locations\n",
    "    business_locations = {}\n",
    "    #iterate over each business in the list\n",
    "    for business in businesses:\n",
    "        #print fetching message for the current business\n",
    "        print(f\"Fetching locations for {business}...\")\n",
    "        #fetch store locations using the get_store_locations function\n",
    "        gdf = get_store_locations(business)\n",
    "        #check if the geodataframe is not empty\n",
    "        if not gdf.empty:\n",
    "            #store the geodataframe in the dictionary\n",
    "            business_locations[business] = gdf\n",
    "            #print the number of locations found\n",
    "            print(f\"Found {len(gdf)} locations for {business}.\")\n",
    "        else:\n",
    "            #print message if no locations are found\n",
    "            print(f\"No locations found for {business}.\")\n",
    "    \n",
    "    #check if any business locations were found\n",
    "    if business_locations:\n",
    "        #combine all store geodataframes into one\n",
    "        all_stores_gdf = gpd.GeoDataFrame(pd.concat(business_locations.values(), ignore_index=True))\n",
    "    else:\n",
    "        #print message and exit if no locations are found\n",
    "        print(\"No store locations found. Exiting script.\")\n",
    "        return\n",
    "\n",
    "    \n",
    "    # Step 3: Load ACS Data and Census Tract Shapefile\n",
    "    \n",
    "    #load acs data from the excel file\n",
    "    acs_df = load_acs_data(acs_excel_path)\n",
    "    #load census tract shapefile\n",
    "    tracts_gdf = load_census_tract_shapefile(tract_shapefile_path)\n",
    "    \n",
    "    #integrate acs data with census tract geometries\n",
    "    acs_tracts_gdf = integrate_acs_with_tracts(acs_df, tracts_gdf)\n",
    "    \n",
    "\n",
    "    # Step 4: Create Analysis Grid\n",
    "\n",
    "    #print message indicating grid creation\n",
    "    print(\"Creating analysis grid...\")\n",
    "    #set minimum x and y coordinates for the grid bounding box\n",
    "    minx, miny = -93.65, 44.6\n",
    "    #set maximum x and y coordinates for the grid bounding box\n",
    "    maxx, maxy = -92.85, 45.3\n",
    "    #create the analysis grid using the specified bounding box and grid spacing\n",
    "    grid_gdf = create_analysis_grid(minx, miny, maxx, maxy, grid_spacing=0.01)\n",
    "    \n",
    "    #assign acs data to grid points based on the census tract they fall into\n",
    "    grid_gdf = assign_acs_data_to_grid(grid_gdf, acs_tracts_gdf)\n",
    "\n",
    "    \n",
    "    # Step 5: Data cleaning\n",
    "    \n",
    "    #load aadt data from the shapefile\n",
    "    aadt_gdf = gpd.read_file(aadt_shapefile_path)\n",
    "    #select relevant columns and drop rows with missing values in 'CURRENT_YE' and 'CURRENT_VO'\n",
    "    aadt_gdf = aadt_gdf[['geometry', 'CURRENT_YE', 'CURRENT_VO']].dropna(subset=['CURRENT_YE', 'CURRENT_VO'])\n",
    "    \n",
    "    #extract features at existing store locations using acs and aadt data\n",
    "    existing_features = extract_features_at_locations(all_stores_gdf, acs_tracts_gdf, aadt_gdf)\n",
    "    #calculate additional features like population density for existing store locations\n",
    "    existing_features = calculate_additional_features(existing_features, acs_tracts_gdf)\n",
    "    \n",
    "    #set the number of negative samples to generate (e.g., 5 times the number of existing features)\n",
    "    num_negative_samples = len(existing_features) * 5  # adjust as needed\n",
    "    #generate negative samples from the grid points\n",
    "    negative_samples = generate_negative_samples(grid_gdf, num_negative_samples)\n",
    "    #calculate average traffic for negative samples within a 1 km radius\n",
    "    negative_samples = calculate_average_traffic(negative_samples, aadt_gdf, radius=1000)\n",
    "    #calculate additional features for negative samples\n",
    "    negative_samples = calculate_additional_features(negative_samples, acs_tracts_gdf)\n",
    "    \n",
    "\n",
    "    # Step 6: Prepare Data for Modeling\n",
    "\n",
    "    #print message indicating data preparation for modeling\n",
    "    print(\"Preparing data for modeling...\")\n",
    "    #define the list of feature column names to be used for modeling\n",
    "    features = [\n",
    "        'average_traffic', 'population_density', 'median_hhi_norm',\n",
    "        'perc_drive_alone', 'perc_public_transit', 'perc_walk_to_work',\n",
    "        'HOMEOWNPCT', 'POVERTYN'\n",
    "    ]\n",
    "    #prepare the training data by combining positive and negative samples\n",
    "    training_data = prepare_training_data(existing_features, negative_samples, features)\n",
    "    #extract feature values from the training data\n",
    "    X = training_data[features].values\n",
    "    #extract labels from the training data\n",
    "    y = training_data['label'].values\n",
    "    \n",
    "    #initialize a MinMaxScaler for feature scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    #fit the scaler and transform the feature values\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    #split the data into training and testing sets with stratification\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "    #print the number of training and testing samples\n",
    "    print(f\"Training samples: {len(X_train)}, Testing samples: {len(X_test)}\")\n",
    "    \n",
    "\n",
    "    # Step 7: Build and Train Neural Network\n",
    "\n",
    "    #build and train the neural network model using the training data\n",
    "    model, history = build_and_train_nn(X_train, y_train, input_dim=X_train.shape[1], epochs=50, batch_size=16)\n",
    "    \n",
    "\n",
    "    # Step 8: Evaluate Model\n",
    "\n",
    "    #evaluate the model using the test data and get feature importances\n",
    "    importance_df = evaluate_model(model, X_test, y_test, features)\n",
    "    \n",
    "\n",
    "    # Step 9: Predict Suitable Locations\n",
    "    \n",
    "    #calculate average traffic for all grid points\n",
    "    grid_gdf = calculate_average_traffic(grid_gdf, aadt_gdf, radius=1000)\n",
    "    #calculate additional features for all grid points\n",
    "    grid_gdf = calculate_additional_features(grid_gdf, acs_tracts_gdf)\n",
    "    #fill any missing values with 0\n",
    "    grid_gdf.fillna(0, inplace=True)\n",
    "    #replace any infinite values with 0\n",
    "    grid_gdf.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    \n",
    "    #predict suitability for all grid points using the trained model\n",
    "    grid_gdf = predict_suitable_locations(model, scaler, grid_gdf, features)\n",
    "    #filter grid points that are predicted as suitable\n",
    "    suitable_gdf = grid_gdf[grid_gdf['suitable'] == 1]\n",
    "    #print the number of suitable locations identified\n",
    "    print(f\"Identified {len(suitable_gdf)} suitable locations.\")\n",
    "\n",
    "\n",
    "    # Step 10: Visualization\n",
    "    \n",
    "    #create an interactive map to visualize existing stores and suitable locations\n",
    "    visualize_results(\n",
    "        stores_gdf=all_stores_gdf,\n",
    "        suitable_gdf=suitable_gdf,\n",
    "        map_filename='retail_suitability_map.html'\n",
    "    )\n",
    "    \n",
    "    #print completion message\n",
    "    print(\"Retail Site Suitability Analysis completed successfully.\")\n",
    "    \n",
    "#execute the script by calling the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec431e1-6475-40d9-b64c-ae7065473e35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
